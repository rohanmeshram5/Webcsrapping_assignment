# -*- coding: utf-8 -*-
"""automation_rohan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MZh2Y3-93zGAuSatBC71sjbF8pphB-rg
"""

# Importing libaries needed for automation and scrapping 
# such as Beautiful soup, requests, csv, pandas, numpy
from bs4 import BeautifulSoup
import requests
import csv
import pandas as pd
import numpy as np

# Opening the csv file rank_artist in write mode. 
csv_file = open("rank_artist.csv","w")

# creating columns as "Week", "Artists" ,"Rank"
csv_writer = csv.writer(csv_file)
csv_writer.writerow(["week","Artists","Rank"])

# getting the urls for different weeks.
url1 =  "https://www.billboard.com/charts/artist-100/2022-04-30/"
url2 =  "https://www.billboard.com/charts/artist-100/2022-04-23/"
url3 =  "https://www.billboard.com/charts/artist-100/2022-04-16/"
url4 =  "https://www.billboard.com/charts/artist-100/2022-04-09/"

# creating thelist of urls.
week = 0
urls = [url1, url2, url3, url4]
for i in urls:
  source = requests.get(i).text
  week += 1

# using the beautiful soup for parsing the data.
  soup = BeautifulSoup(source, 'lxml')
  tables = soup.find_all("div", class_= "o-chart-results-list-row-container")
  for table in tables:
    #  Getting the artist name 
    artist = table.find("h3", class_= "c-title").text.strip()
    #  Getting the rank of the artist
    rank = table.find("span", class_="c-label").text.strip()
  
    # writting row by row in the csv file.
    csv_writer.writerow([week, artist, rank])
# Closing the csv file.    
csv_file.close()

# Reading the csv file.
df = pd.read_csv("/content/rank_artist.csv")

top_5_week1 = df[:5]
display(top_5_week1)

top_5_week2 = df[100:105]
display(top_5_week2)

top_5_week3 = df[200:205]
display(top_5_week3)

top_5_week4 = df[300:305]
display(top_5_week4)

top5_all = pd.concat([top_5_week1, top_5_week2, top_5_week3, top_5_week4], join="inner")

display(top5_all)

top5_artists_for_4_weeks = top5_all["Artists"]
display(top5_artists_for_4_weeks)

top5_artists_for_4_weeks_unique = top5_artists_for_4_weeks.unique()
display(top5_artists_for_4_weeks_unique)


top5_artists_for_4_weeks_unique_aphabetical_order = np.sort(top5_artists_for_4_weeks_unique)
display(top5_artists_for_4_weeks_unique_aphabetical_order)

# Creating the dataframe from the data and naming the colunm as Artist
df1 = pd.DataFrame(top5_artists_for_4_weeks_unique_aphabetical_order, columns = ['Artists'])

print(df1)
print(type(df1))

# creating a out.csv file from the dataframe df1.
df1.to_csv("out.csv",index=False)


''' 
# Code for form generation in google script.


function form_gen() {
  let spred_sheet = SpreadsheetApp.openByUrl("https://docs.google.com/spreadsheets/d/1yqNYBEAt0a0dqnUdFBDbiiswZ0RHxTr2nBl2Ww_XnsY/edit#gid=324803724");
  let values = spred_sheet.getRange("A2:A13").getValues();
  
  let form = FormApp.create("user_rating");
  for (let row of values){
    let item = form.addMultipleChoiceItem();
    item.setTitle(row)
    item.setChoices([
      item.createChoice(1),
      item.createChoice(2),
      item.createChoice(3),
      item.createChoice(4),
      item.createChoice(5),
    ]);
  }  
  MailApp.sendEmail("rohanmeshram1987@zohomail.in", "please fill this form", "here is the form : " + form.getPublishedUrl());
}  

'''

'''
Tried running this code for upoading the file but getting into errors and have less time for submission so skipping it.

from googleapiclient.http import MediaFileUpload
from Google import Create_Service

CLIENT_SECRET_FILE = 'client_server_GoogleCloudDemo.json'
API_NAME = 'drive'
API_VERSION = 'v3'
SCOPES = ['https://www.googleapis.com/auth/drive']

service = Create_Service(CLIENT_SECRET_FILE, API_NAME, API_VERSION, SCOPES)

folder_id = "1vdcQWg0IagRWA1FKMzWSNjqZPUxGEQvd"
file_names = ["wildhorse.csv"]
mime_types = ["text/csv"]

for file_name, mime_type in zip(file_names, mime_types):
  file_metadata = {
      'name' : file_name,
      'parents' : [folder_id]
  }

  media = MediaFileUpload('./los/{0}'.format(file_name), mimetype= mime_type)

  service.files().create(
      body = file_metadata,
      media_body = media,
      fields = 'id'
  ).execute()
'''